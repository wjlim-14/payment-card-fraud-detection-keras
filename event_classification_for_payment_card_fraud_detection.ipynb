{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjlim-14/payment-card-fraud-detection-keras/blob/main/event_classification_for_payment_card_fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y-Tloo0d0iu"
      },
      "source": [
        "# Event classification for payment card fraud detection\n",
        "\n",
        "**Author:** [achoum](https://github.com/achoum/)<br>\n",
        "**Date created:** 2024/02/01<br>\n",
        "**Last modified:** 2024/02/01<br>\n",
        "**Description:** Detection of fraudulent payment card transactions using Temporian and a feed-forward neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSOqUxT6d0iv"
      },
      "source": [
        "This notebook depends on Keras 3, Temporian, and a few other libraries. You can\n",
        "install them as follow:\n",
        "\n",
        "```shell\n",
        "pip install temporian keras pandas tf-nightly scikit-learn -U\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfoFBXJ_d0iv"
      },
      "outputs": [],
      "source": [
        "import keras  # To train the Machine Learning model\n",
        "import temporian as tp  # To convert transactions into tabular data\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import RocCurveDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2ZRA4H-d0iw"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Payment fraud detection is critical for banks, businesses, and consumers. In\n",
        "Europe alone, fraudulent transactions were estimated at\n",
        "[â‚¬1.89 billion in 2019](https://www.ecb.europa.eu/pub/pdf/cardfraud/ecb.cardfraudreport202110~cac4c418e8.en.pdf).\n",
        "Worldwide, approximately\n",
        "[3.6%](https://www.cybersource.com/content/dam/documents/campaign/fraud-report/global-fraud-report-2022.pdf)\n",
        "of commerce revenue is lost to fraud. In this notebook, we train and evaluate a\n",
        "model to detect fraudulent transactions using the synthetic dataset attached to\n",
        "the book\n",
        "[Reproducible Machine Learning for Credit Card Fraud Detection](https://fraud-detection-handbook.github.io/fraud-detection-handbook/Foreword.html)\n",
        "by Le Borgne et al.\n",
        "\n",
        "Fraudulent transactions often cannot be detected by looking at transactions in\n",
        "isolation. Instead, fraudulent transactions are detected by looking at patterns\n",
        "across multiple transactions from the same user, to the same merchant, or with\n",
        "other types of relationships. To express these relationships in a way that is\n",
        "understandable by a machine learning model, and to augment features with feature\n",
        " engineering, we We use the\n",
        " [Temporian](https://temporian.readthedocs.io/en/latest) preprocessing library.\n",
        "\n",
        "We preprocess a transaction dataset into a tabular dataset and use a\n",
        "feed-forward neural network to learn the patterns of fraud and make predictions.\n",
        "\n",
        "## Loading the dataset\n",
        "\n",
        "The dataset contains payment transactions sampled between April 1, 2018 and\n",
        "September 30, 2018. The transactions are stored in CSV files, one for each day.\n",
        "\n",
        "**Note:** Downloading the dataset takes ~1 minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4uZ61bhd0iw"
      },
      "outputs": [],
      "source": [
        "start_date = datetime.date(2018, 4, 1)\n",
        "end_date = datetime.date(2018, 9, 30)\n",
        "\n",
        "# Load the dataset as a Pandas dataframe.\n",
        "cache_path = \"fraud_detection_cache.csv\"\n",
        "if not os.path.exists(cache_path):\n",
        "    print(\"Download dataset\")\n",
        "    dataframes = []\n",
        "    num_files = (end_date - start_date).days\n",
        "    counter = 0\n",
        "    while start_date <= end_date:\n",
        "        if counter % (num_files // 10) == 0:\n",
        "            print(f\"[{100 * (counter+1) // num_files}%]\", end=\"\", flush=True)\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        url = f\"https://github.com/Fraud-Detection-Handbook/simulated-data-raw/raw/6e67dbd0a3bfe0d7ec33abc4bce5f37cd4ff0d6a/data/{start_date}.pkl\"\n",
        "        dataframes.append(pd.read_pickle(url))\n",
        "        start_date += datetime.timedelta(days=1)\n",
        "        counter += 1\n",
        "    print(\"done\", flush=True)\n",
        "    transactions_dataframe = pd.concat(dataframes)\n",
        "    transactions_dataframe.to_csv(cache_path, index=False)\n",
        "else:\n",
        "    print(\"Load dataset from cache\")\n",
        "    transactions_dataframe = pd.read_csv(\n",
        "        cache_path, dtype={\"CUSTOMER_ID\": bytes, \"TERMINAL_ID\": bytes}\n",
        "    )\n",
        "\n",
        "print(f\"Found {len(transactions_dataframe)} transactions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TNaY0tgd0iw"
      },
      "source": [
        "Each transaction is represented by a single row, with the following columns of\n",
        "interest:\n",
        "\n",
        "- **TX_DATETIME**: The date and time of the transaction.\n",
        "- **CUSTOMER_ID**: The unique identifier of the customer.\n",
        "- **TERMINAL_ID**: The identifier of the terminal where the transaction was\n",
        "    made.\n",
        "- **TX_AMOUNT**: The amount of the transaction.\n",
        "- **TX_FRAUD**: Whether the transaction is fraudulent (1) or not (0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIhi1M2ud0iw"
      },
      "outputs": [],
      "source": [
        "transactions_dataframe = transactions_dataframe[\n",
        "    [\"TX_DATETIME\", \"CUSTOMER_ID\", \"TERMINAL_ID\", \"TX_AMOUNT\", \"TX_FRAUD\"]\n",
        "]\n",
        "\n",
        "transactions_dataframe.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZK2dNOrd0ix"
      },
      "source": [
        "The dataset is highly imbalanced, with the majority of transactions being\n",
        "legitimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5EMJJkSd0ix"
      },
      "outputs": [],
      "source": [
        "fraudulent_rate = transactions_dataframe[\"TX_FRAUD\"].mean()\n",
        "print(\"Rate of fraudulent transactions:\", fraudulent_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "660RhibPd0ix"
      },
      "source": [
        "The\n",
        "[pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)\n",
        "is converted into a\n",
        "[Temporian EventSet](https://temporian.readthedocs.io/en/latest/reference/temporian/EventSet/),\n",
        "which is better suited for the data exploration and feature preprocessing of the\n",
        " next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS5PS55dd0ix"
      },
      "outputs": [],
      "source": [
        "transactions_evset = tp.from_pandas(transactions_dataframe, timestamps=\"TX_DATETIME\")\n",
        "\n",
        "transactions_evset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zKURTu0d0ix"
      },
      "source": [
        "It is possible to plot the entire dataset, but the resulting plot will be\n",
        "difficult to read. Instead, we can group the transactions per client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_C5GZb4d0ix"
      },
      "outputs": [],
      "source": [
        "transactions_evset.add_index(\"CUSTOMER_ID\").plot(indexes=\"3774\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnWbJlcwd0ix"
      },
      "source": [
        "Note the few fraudulent transactions for this client.\n",
        "\n",
        "## Preparing the training data\n",
        "\n",
        "Fraudulent transactions in isolation cannot be detected. Instead, we need to\n",
        "connect related transactions. For each transaction, we compute the sum and count\n",
        "of transactions for the same terminal in the last `n` days. Because we don't\n",
        "know the correct value for `n`, we use multiple values for `n` and compute a\n",
        "set of features for each of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxBqP6qKd0ix"
      },
      "outputs": [],
      "source": [
        "# Group the transactions per terminal\n",
        "transactions_per_terminal = transactions_evset.add_index(\"TERMINAL_ID\")\n",
        "\n",
        "# Moving statistics per terminal\n",
        "tmp_features = []\n",
        "for n in [7, 14, 28]:\n",
        "    tmp_features.append(\n",
        "        transactions_per_terminal[\"TX_AMOUNT\"]\n",
        "        .moving_sum(tp.duration.days(n))\n",
        "        .rename(f\"sum_transactions_{n}_days\")\n",
        "    )\n",
        "\n",
        "    tmp_features.append(\n",
        "        transactions_per_terminal.moving_count(tp.duration.days(n)).rename(\n",
        "            f\"count_transactions_{n}_days\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "feature_set_1 = tp.glue(*tmp_features)\n",
        "\n",
        "feature_set_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2nBJq3Ad0ix"
      },
      "source": [
        "Let's look at the features of terminal \"3774\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkEv-sLMd0ix"
      },
      "outputs": [],
      "source": [
        "feature_set_1.plot(indexes=\"3774\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d_PC03yd0iy"
      },
      "source": [
        "A transaction's fraudulent status is not known at the time of the transaction\n",
        "(otherwise, there would be no problem). However, the banks knows if a\n",
        "transacation is fraudulent one week after it is made. We create a set of\n",
        "features that indicate the number and ratio of fraudulent transactions in the\n",
        "last N days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUap9Cajd0iy"
      },
      "outputs": [],
      "source": [
        "# Lag the transactions by one week.\n",
        "lagged_transactions = transactions_per_terminal.lag(tp.duration.weeks(1))\n",
        "\n",
        "# Moving statistics per customer\n",
        "tmp_features = []\n",
        "for n in [7, 14, 28]:\n",
        "    tmp_features.append(\n",
        "        lagged_transactions[\"TX_FRAUD\"]\n",
        "        .moving_sum(tp.duration.days(n), sampling=transactions_per_terminal)\n",
        "        .rename(f\"count_fraud_transactions_{n}_days\")\n",
        "    )\n",
        "\n",
        "    tmp_features.append(\n",
        "        lagged_transactions[\"TX_FRAUD\"]\n",
        "        .cast(tp.float32)\n",
        "        .simple_moving_average(tp.duration.days(n), sampling=transactions_per_terminal)\n",
        "        .rename(f\"rate_fraud_transactions_{n}_days\")\n",
        "    )\n",
        "\n",
        "feature_set_2 = tp.glue(*tmp_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg9YsSdld0iy"
      },
      "source": [
        "Transaction date and time can be correlated with fraud. While each transaction\n",
        "has a timestamp, a machine learning model might struggle to consume them\n",
        "directly. Instead, we extract various informative calendar features from the\n",
        "timestamps, such as hour, day of the week (e.g., Monday, Tuesday), and day of\n",
        "the month (1-31)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8WaOkASd0iy"
      },
      "outputs": [],
      "source": [
        "feature_set_3 = tp.glue(\n",
        "    transactions_per_terminal.calendar_hour(),\n",
        "    transactions_per_terminal.calendar_day_of_week(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf5fhAV-d0iy"
      },
      "source": [
        "Finally, we group together all the features and the label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvlr21m9d0iy"
      },
      "outputs": [],
      "source": [
        "all_data = tp.glue(\n",
        "    transactions_per_terminal, feature_set_1, feature_set_2, feature_set_3\n",
        ").drop_index()\n",
        "\n",
        "print(\"All the available features:\")\n",
        "all_data.schema.feature_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl33wjjCd0iy"
      },
      "source": [
        "We extract the name of the input features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDuaLm9Hd0iy"
      },
      "outputs": [],
      "source": [
        "input_feature_names = [k for k in all_data.schema.feature_names() if k.islower()]\n",
        "\n",
        "print(\"The model's input features:\")\n",
        "input_feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgAFmmlJd0iy"
      },
      "source": [
        "For neural networks to work correctly, numerical inputs must be normalized. A\n",
        "common approach is to apply z-normalization, which involves subtracting the mean\n",
        "and dividing by the standard deviation estimated from the training data to each\n",
        "value. In forecasting, such z-normalization is not recommended as it would lead\n",
        "to future leakage. Specifically, to classify a transaction at time t, we cannot\n",
        "rely on data after time t since, at serving time when making a prediction at\n",
        "time t, no subsequent data is available yet. In short, at time t, we are limited\n",
        "to using data that precedes or is concurrent with time t.\n",
        "\n",
        "The solution is therefore to apply z-normalization **over time**, which means\n",
        "that we normalize each transaction using the mean and standard deviation\n",
        "computed from the past data **for that transaction**.\n",
        "\n",
        "Future leakage is pernicious. Luckily, Temporian is here to help: the only\n",
        "operator that can cause future leakage is `EventSet.leak()`. If you are not\n",
        "using `EventSet.leak()`, your preprocessing is **guaranteed** not to create\n",
        "future leakage.\n",
        "\n",
        "**Note:** For advanced pipelines, you can also check programatically that a\n",
        "feature does not depends on an `EventSet.leak()` operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlJ3aq0vd0iy"
      },
      "outputs": [],
      "source": [
        "# Cast all values (e.g. ints) to floats.\n",
        "values = all_data[input_feature_names].cast(tp.float32)\n",
        "\n",
        "# Apply z-normalization overtime.\n",
        "normalized_features = (\n",
        "    values - values.simple_moving_average(math.inf)\n",
        ") / values.moving_standard_deviation(math.inf)\n",
        "\n",
        "# Restore the original name of the features.\n",
        "normalized_features = normalized_features.rename(values.schema.feature_names())\n",
        "\n",
        "print(normalized_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADJI933Rd0iy"
      },
      "source": [
        "The first transactions will be normalized using poor estimates of the mean and\n",
        "standard deviation since there are only a few transactions before them. To\n",
        "mitigate this issue, we remove the first week of data from the training dataset.\n",
        "\n",
        "Notice that the first values contain NaN. In Temporian, NaN represents missing\n",
        "values, and all operators handle them accordingly. For instance, when\n",
        "calculating a moving average, NaN values are not included in the calculation\n",
        "and do not generate a NaN result.\n",
        "\n",
        "However, neural networks cannot natively handle NaN values. So, we replace them\n",
        "with zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMt-QiCpd0iy"
      },
      "outputs": [],
      "source": [
        "normalized_features = normalized_features.fillna(0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4F1SMrpd0iy"
      },
      "source": [
        "Finally, we group together the features and the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0yDeINld0iy"
      },
      "outputs": [],
      "source": [
        "normalized_all_data = tp.glue(normalized_features, all_data[\"TX_FRAUD\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEag7iaad0iy"
      },
      "source": [
        "## Split dataset into a train, validation and test set\n",
        "\n",
        "To evaluate the quality of our machine learning model, we need training,\n",
        "validation and test sets. Since the system is dynamic (new fraud patterns are\n",
        "being created all the time), it is important for the training set to come before\n",
        "the validation set, and the validation set come before the testing set:\n",
        "\n",
        "- **Training:** April 8, 2018 to July 31, 2018\n",
        "- **Validation:** August 1, 2018 to August 31, 2018\n",
        "- **Testing:** September 1, 2018 to September 30, 2018\n",
        "\n",
        "For the example to run faster, we will effectively reduce the size of the\n",
        "training set to:\n",
        "- **Training:** July 1, 2018 to July 31, 2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNLCoS8ed0iy"
      },
      "outputs": [],
      "source": [
        "# begin_train = datetime.datetime(2018, 4, 8).timestamp() # Full training dataset\n",
        "begin_train = datetime.datetime(2018, 7, 1).timestamp()  # Reduced training dataset\n",
        "begin_valid = datetime.datetime(2018, 8, 1).timestamp()\n",
        "begin_test = datetime.datetime(2018, 9, 1).timestamp()\n",
        "\n",
        "is_train = (normalized_all_data.timestamps() >= begin_train) & (\n",
        "    normalized_all_data.timestamps() < begin_valid\n",
        ")\n",
        "is_valid = (normalized_all_data.timestamps() >= begin_valid) & (\n",
        "    normalized_all_data.timestamps() < begin_test\n",
        ")\n",
        "is_test = normalized_all_data.timestamps() >= begin_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKy1aKmRd0iy"
      },
      "source": [
        "`is_train`, `is_valid` and `is_test` are boolean features overtime that indicate\n",
        "the limit of the tree folds. Let's plot them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY6q8KcMd0iy"
      },
      "outputs": [],
      "source": [
        "tp.plot(\n",
        "    [\n",
        "        is_train.rename(\"is_train\"),\n",
        "        is_valid.rename(\"is_valid\"),\n",
        "        is_test.rename(\"is_test\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84iUA6dRd0iy"
      },
      "source": [
        "We filter the input features and label in each fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsipuLZVd0iy"
      },
      "outputs": [],
      "source": [
        "train_ds_evset = normalized_all_data.filter(is_train)\n",
        "valid_ds_evset = normalized_all_data.filter(is_valid)\n",
        "test_ds_evset = normalized_all_data.filter(is_test)\n",
        "\n",
        "print(f\"Training examples: {train_ds_evset.num_events()}\")\n",
        "print(f\"Validation examples: {valid_ds_evset.num_events()}\")\n",
        "print(f\"Testing examples: {test_ds_evset.num_events()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFV4h4bgd0iy"
      },
      "source": [
        "It is important to split the dataset **after** the features have been computed\n",
        "because some of the features for the training dataset are computed from\n",
        "transactions during the training window.\n",
        "\n",
        "## Create TensorFlow datasets\n",
        "\n",
        "We convert the datasets from EventSets to TensorFlow Datasets as Keras consumes\n",
        "them natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQk9mZ4zd0iy"
      },
      "outputs": [],
      "source": [
        "non_batched_train_ds = tp.to_tensorflow_dataset(train_ds_evset)\n",
        "non_batched_valid_ds = tp.to_tensorflow_dataset(valid_ds_evset)\n",
        "non_batched_test_ds = tp.to_tensorflow_dataset(test_ds_evset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JY29RZjd0i1"
      },
      "source": [
        "The following processing steps are applied using TensorFlow datasets:\n",
        "\n",
        "1. The features and labels are separated using `extract_features_and_label` in\n",
        "    the format that Keras expects.\n",
        "1. The dataset is batched, which means that the examples are grouped into\n",
        "    mini-batches.\n",
        "1. The training examples are shuffled to improve the quality of mini-batch\n",
        "    training.\n",
        "\n",
        "As we noted before, the dataset is imbalanced in the direction of legitimate\n",
        "transactions. While we want to evaluate our model on this original distribution,\n",
        "neural networks often train poorly on strongly imbalanced datasets. Therefore,\n",
        "we resample the training dataset to a ratio of 80% legitimate / 20% fraudulent\n",
        "using `rejection_resample`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5mvUZdjd0i1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_features_and_label(example):\n",
        "    features = {k: example[k] for k in input_feature_names}\n",
        "    labels = tf.cast(example[\"TX_FRAUD\"], tf.int32)\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "# Target ratio of fraudulent transactions in the training dataset.\n",
        "target_rate = 0.2\n",
        "\n",
        "# Number of examples in a mini-batch.\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = (\n",
        "    non_batched_train_ds.shuffle(10000)\n",
        "    .rejection_resample(\n",
        "        class_func=lambda x: tf.cast(x[\"TX_FRAUD\"], tf.int32),\n",
        "        target_dist=[1 - target_rate, target_rate],\n",
        "        initial_dist=[1 - fraudulent_rate, fraudulent_rate],\n",
        "    )\n",
        "    .map(lambda _, x: x)  # Remove the label copy added by \"rejection_resample\".\n",
        "    .batch(batch_size)\n",
        "    .map(extract_features_and_label)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# The test and validation dataset does not need resampling or shuffling.\n",
        "valid_ds = (\n",
        "    non_batched_valid_ds.batch(batch_size)\n",
        "    .map(extract_features_and_label)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "test_ds = (\n",
        "    non_batched_test_ds.batch(batch_size)\n",
        "    .map(extract_features_and_label)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WNzl6dXd0i1"
      },
      "source": [
        "We print the first four examples of the training dataset. This is a simple way\n",
        "to identify some of the errors that could have been made above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5UB7QPad0i1"
      },
      "outputs": [],
      "source": [
        "for features, labels in train_ds.take(1):\n",
        "    print(\"features\")\n",
        "    for feature_name, feature_value in features.items():\n",
        "        print(f\"\\t{feature_name}: {feature_value[:4]}\")\n",
        "    print(f\"labels: {labels[:4]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BRMZ9Zmd0i1"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "The original dataset is transactional, but the processed data is tabular and\n",
        "only contains normalized numerical values. Therefore, we train a feed-forward\n",
        "neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXRPrl0jd0i1"
      },
      "outputs": [],
      "source": [
        "inputs = [keras.Input(shape=(1,), name=name) for name in input_feature_names]\n",
        "x = keras.layers.concatenate(inputs)\n",
        "x = keras.layers.Dense(32, activation=\"sigmoid\")(x)\n",
        "x = keras.layers.Dense(16, activation=\"sigmoid\")(x)\n",
        "x = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTExrzzcd0i1"
      },
      "source": [
        "Our goal is to differentiate between the fraudulent and legitimate transactions,\n",
        "so we use a binary classification objective. Because the dataset is imbalanced,\n",
        "accuracy is not an informative metric. Instead, we evaluate the model using the\n",
        "[area under the curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)\n",
        "(AUC)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLC7uJiDd0i1"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(0.01),\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[keras.metrics.Accuracy(), keras.metrics.AUC()],\n",
        ")\n",
        "model.fit(train_ds, validation_data=valid_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcIeoWUZd0i2"
      },
      "source": [
        "We evaluate the model on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS16pkZhd0i2"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvi1BftId0i2"
      },
      "source": [
        "With and AUC of ~83%, our simple fraud detector is showing encouraging\n",
        "results.\n",
        "\n",
        "\n",
        "Plotting the ROC curve is a good solution to understand and select the operation\n",
        "point of the model i.e. the threshold applied on the model output to\n",
        "differentiate between fraudulent and legitimate transactions.\n",
        "\n",
        "Compute the test predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCGfpzrxd0i2"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_ds)\n",
        "predictions = np.nan_to_num(predictions, nan=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JISXZY4Sd0i2"
      },
      "source": [
        "Extract the labels from the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXzX7fZYd0i2"
      },
      "outputs": [],
      "source": [
        "labels = np.concatenate([label for _, label in test_ds])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUQbVcypd0i2"
      },
      "source": [
        "Finaly, we plot the ROC curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdCeea_Ud0i2"
      },
      "outputs": [],
      "source": [
        "_ = RocCurveDisplay.from_predictions(labels, predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twf-kPZRd0i2"
      },
      "source": [
        "The Keras model is ready to be used on transactions with an unknown fraud\n",
        "status, a.k.a. serving. We save the model on disk for future use.\n",
        "\n",
        "**Note:** The model does not include the data preparation and preprocessing steps\n",
        "done in Pandas and Temporian. They have to be applied manually to the data fed\n",
        "into the model. While not demonstrated here, Temporian preprocessing can also be\n",
        "saved to disk with\n",
        "[tp.save](https://temporian.readthedocs.io/en/latest/reference/temporian/serialization/save/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZFFSN8bd0i2"
      },
      "outputs": [],
      "source": [
        "model.save(\"fraud_detection_model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZwK7BHQd0i3"
      },
      "source": [
        "The model can be later reloaded with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqvahyWkd0i3"
      },
      "outputs": [],
      "source": [
        "loaded_model = keras.saving.load_model(\"fraud_detection_model.keras\")\n",
        "\n",
        "# Generate predictions with the loaded model on 5 test examples.\n",
        "loaded_model.predict(test_ds.rebatch(5).take(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9_Lu3Rfd0i3"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "We trained a feed-forward neural network to identify fraudulent transactions. To\n",
        "feed them into the model, the transactions were preprocessed and transformed\n",
        "into a tabular dataset using\n",
        "[Temporian](https://temporian.readthedocs.io/en/latest/). Now, a question to the\n",
        "reader: What could be done to further improve the model's performance?\n",
        "\n",
        "Here are some ideas:\n",
        "\n",
        "- Train the model on the entire dataset instead of a single month of data.\n",
        "- Train the model for more epochs and use early stopping to ensure that the\n",
        "    model is fully trained without overfitting.\n",
        "- Make the feed-forward network more powerful by increasing the number of layers\n",
        "    while ensuring that the model is regularized.\n",
        "- Compute additional preprocessing features. For example, in addition to\n",
        "    aggregating transactions by terminal, aggregate transactions by client.\n",
        "- Use the Keras Tuner to perform hyperparameter tuning on the model. Note that\n",
        "    the parameters of the preprocessing (e.g., the number of days of\n",
        "    aggregations) are also hyperparameters that can be tuned."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}